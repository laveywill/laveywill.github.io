[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/classifying-penguins/index.html",
    "href": "posts/classifying-penguins/index.html",
    "title": "Classifying Palmer’s Penguins",
    "section": "",
    "text": "The aim of this blog post is to explore the relationships between predictor variables and the species of the penguins found in the famous Palmer’s Penguins dataset. The analysis was conducted through a series of data preparation, exploratory data analysis and visualization, feature selection, and finally model training and testing. These processes coincide to form a model that boasts perfect prediction accuracy for penguin species in the testing data.\n\n\n\nWe’ll begin by reading in the data and doing some data preparation.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nLet’s view the dataset for better understanding.\n\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n256 rows × 14 columns"
  },
  {
    "objectID": "posts/classifying-penguins/index.html#abstract",
    "href": "posts/classifying-penguins/index.html#abstract",
    "title": "Classifying Palmer’s Penguins",
    "section": "",
    "text": "The aim of this blog post is to explore the relationships between predictor variables and the species of the penguins found in the famous Palmer’s Penguins dataset. The analysis was conducted through a series of data preparation, exploratory data analysis and visualization, feature selection, and finally model training and testing. These processes coincide to form a model that boasts perfect prediction accuracy for penguin species in the testing data."
  },
  {
    "objectID": "posts/classifying-penguins/index.html#data-preparation",
    "href": "posts/classifying-penguins/index.html#data-preparation",
    "title": "Classifying Palmer’s Penguins",
    "section": "",
    "text": "We’ll begin by reading in the data and doing some data preparation.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nLet’s view the dataset for better understanding.\n\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n256 rows × 14 columns"
  },
  {
    "objectID": "posts/classifying-penguins/index.html#model-and-feature-selection",
    "href": "posts/classifying-penguins/index.html#model-and-feature-selection",
    "title": "Classifying Palmer’s Penguins",
    "section": "Model and Feature Selection",
    "text": "Model and Feature Selection\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\nall_qual_cols = X_train.select_dtypes(include='bool')\nall_quant_cols = X_train.select_dtypes(include=np.number)\nbest_LR_score = 0\nbest_SVM_score = 0\nbest_LR_cols = []\nbest_SVM_cols = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    SVM = SVC()\n    SVM.fit(X_train[cols], y_train)\n    \n    LR_score = cross_val_score(LR, X_train[cols], y_train, cv = 5).mean()\n    SVM_score = cross_val_score(SVM, X_train[cols], y_train, cv = 5).mean()\n    \n    if LR_score &gt; best_LR_score:\n      best_LR_cols = cols\n      best_LR_score = LR_score\n    if SVM_score &gt; best_SVM_score:\n      best_SVM_cols = cols\n      best_SVM_score = SVM_score\n\n\nprint(\"Best LR Score: \", best_LR_score,\n      \"\\n Best SVM Score: \", best_SVM_score)\n\nBest LR Score:  0.9922322775263952 \n Best SVM Score:  0.890573152337858\n\n\n\nprint(\"Best LR Columns: \", best_LR_cols)\n\nBest LR Columns:  ['Island_Dream', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nFrom this model selection process, we can conclude that the logistic regression model process is favorable for this prediction task as it boasts a mean accuracy over 5 cross-validation folds of \\(0.9922\\). We will move forward with the best columns associated with the greater accuracy of the logistic regression model, being ‘Island’, ‘Culmen Length’, ‘Culmen Depth’. We will use all three variables derived from island association.\n\nselected_cols = [\"Culmen Length (mm)\", \n                 \"Culmen Depth (mm)\",\n                 \"Island_Dream\", \n                 \"Island_Biscoe\",\n                 \"Island_Torgersen\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[selected_cols], y_train)\nscores = cross_val_score(LR, X_train[selected_cols], y_train, cv = 5)\nscores\n\narray([0.981, 1.   , 1.   , 1.   , 1.   ])\n\n\nUsing all selected features, including island association variables, the Logistic Regression model nearly boasts perfect accuracy on the training data sample. Let’s see how it performs on the testing data.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[selected_cols], y_test)\n\n1.0\n\n\nWow! A perfect score! Exquisite modeling! Let’s visualize our model findings."
  },
  {
    "objectID": "posts/classifying-penguins/index.html#final-model-visualizations",
    "href": "posts/classifying-penguins/index.html#final-model-visualizations",
    "title": "Classifying Palmer’s Penguins",
    "section": "Final Model Visualizations",
    "text": "Final Model Visualizations\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[selected_cols], y_train)\n\n\n\n\n\n\n\n\nThe plot_regions() function provided to us allows us to visualize the model’s decisions based on the selected features. The view is faceted by the dummy variables previously created for the Island column. These visualizations of the data help to inform us how the data are being split in the Logistic Regression model. On Dream and Biscoe Island, different species live on both, thus making the classification job more difficult. This is accounted for using the measurements for the culmen length and depth, which help to separate the data as visualized in the plots above. Torgersen Island only house Gentoo penguins, and thus the culmen length and depth measurements are unneccesary to split the data further."
  },
  {
    "objectID": "posts/classifying-penguins/index.html#confusion-matrix",
    "href": "posts/classifying-penguins/index.html#confusion-matrix",
    "title": "Classifying Palmer’s Penguins",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[selected_cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nThis confusion matrix allows us to view in which accuracy categories our model excels and which it underperforms. Fortunately, the model and feature selection processes result in perfect accuracy, in which the model does not make any wrong species predictions for the test data."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Will Lavey’s Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer’s Penguins\n\n\n\n\n\nThe first blog post for CS451, classifying Palmer’s Penguins\n\n\n\n\n\nFeb 28, 2025\n\n\nWill Lavey\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]